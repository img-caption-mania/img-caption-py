{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "checkpoint_path = PATH + \"/checkpoints/train\"\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "EPOCHS = 250\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "top_k = 200\n",
    "vocab_size = top_k + 1\n",
    "num_steps = 384 // BATCH_SIZE  # 384 / 64 = 6 # from 80% of data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH + \"/all_caption.json\", 'r') as f:\n",
    "  annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_capImg(annotations, num_examples=480):\n",
    "\n",
    "    # Store captions and image names in list\n",
    "    all_captions = []\n",
    "    all_img_name_vector = []\n",
    "\n",
    "    for annot in annotations:\n",
    "        caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "        image_id = annot['image_id']\n",
    "        full_image_path = PATH + '/dataset/{IMG}'.format(IMG=image_id)\n",
    "        all_img_name_vector.append(full_image_path)\n",
    "        all_captions.append(caption)\n",
    "\n",
    "    # Shuffle captions and image_names together\n",
    "    # Set a random state\n",
    "    train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                              all_img_name_vector,\n",
    "                                              random_state=1)\n",
    "\n",
    "    train_captions = train_captions[:num_examples]\n",
    "    img_name_vector = img_name_vector[:num_examples]\n",
    "\n",
    "    return train_captions, img_name_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_featExtract():\n",
    "  # initiate image feature extractor model\n",
    "\n",
    "    image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                    weights='imagenet')\n",
    "\n",
    "    new_input = image_model.input\n",
    "\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "    return image_features_extract_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "  \n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_feature(img_name_vector, image_features_extract_model):\n",
    "  # - `store the resulting vector` `in a dictionary` (image_name --> feature_vector).\n",
    "  # - you pickle the dictionary and save it to disk.\n",
    "\n",
    "    encode_train = sorted(set(img_name_vector))\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "    image_dataset = image_dataset.map(\n",
    "      load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16) # map functional programming\n",
    "\n",
    "    for img, path in tqdm(image_dataset):\n",
    "      batch_features = image_features_extract_model(img) # image features extractor model\n",
    "      # print(\"batch_features, batch_features.shape[0], -1, batch_features.shape[3] in order\",batch_features, batch_features.shape[0], -1, batch_features.shape[3])\n",
    "\n",
    "      batch_features = tf.reshape(batch_features,\n",
    "                                  (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "      for bf, p in zip(batch_features, path): # bf : batch feature , p : path\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_cap(train_captions):\n",
    "    # Choose the top 5000 words from the vocabulary\n",
    "    top_k = 5000\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                      oov_token=\"<unk>\",\n",
    "                                                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "    tokenizer.fit_on_texts(train_captions)\n",
    "    train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_cap(tokenizer, train_captions):\n",
    "\n",
    "    # Create the tokenized vectors\n",
    "    train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "    # Pad each vector to the max_length of the captions\n",
    "    # If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "    return cap_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npFile(img_name_train, cap_train, BATCH_SIZE, BUFFER_SIZE):\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "    # Use map to load the numpy files in parallel\n",
    "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "              map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Shuffle and batch\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim) 64, 64, 256\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)  64, 512\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size) (64, 1, 512)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 64, hidden_size)\n",
    "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)) # vector_size 512 + vector_size 512\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1) # from 512 --> 1 fc\n",
    "    # (64, 64, 1)\n",
    "    # you get 1 at the last axis because you are applying score to self.V\n",
    "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "    # print(\"attention_weights: \",attention_weights)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features # multiplication feature x attention\n",
    "    # context_vector:  Tensor(\"rnn__decoder_1/bahdanau_attention_1/mul:0\", shape=(64, 64, 256), dtype=float32)\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1) # vector sum\n",
    "    # context_vector:  Tensor(\"rnn__decoder_11/bahdanau_attention/Sum:0\", shape=(64, 256), dtype=float32)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model): # init CNN_Encoder(embedding_dim)\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # (8, 8, 2048)\n",
    "        # (64, 2048)\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        # (64, 256)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim) # embedding_dim = 256\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model): # init RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units                          # hidden units 512\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # (5001, 256)\n",
    "\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # embedding --> concat \\w context_vector attention --> GRU --> fc1 \\w output (to predict words) 512 --> reshape x (to predict words) --> fc1 \\w x (to predict words) 5001 (vocab classification) \n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)  # 256 + hidden_size(context_vector)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)  # output to predicted words and state to next GRU layer unit\n",
    "    # output:  Tensor(\"rnn__decoder_2/gru_2/transpose_1:0\", shape=(64, 1, 512), dtype=float32)\n",
    "    # state:  Tensor(\"rnn__decoder_2/gru_2/while:4\", shape=(64, 512), dtype=float32)    \n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size) # maximum_length of caption sentence = 13, hidden_sz = 512\n",
    "    x = self.fc1(output)\n",
    "    # x fc1 output:  Tensor(\"rnn__decoder_2/dense_13/BiasAdd:0\", shape=(64, 1, 512), dtype=float32)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "    # x reshape:  Tensor(\"rnn__decoder_2/Reshape:0\", shape=(64, 512), dtype=float32)\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab) # vocab = 201 need to be adjust to own vocab size\n",
    "    x = self.fc2(x)\n",
    "    # x fc2:  Tensor(\"rnn__decoder_2/dense_14/BiasAdd:0\", shape=(64, 201), dtype=float32)\n",
    "\n",
    "    return x, state, attention_weights    # x for predicted words class , state for GRU next hidden layer, attention_weights ???\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units)) # 64 x 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred, loss_object):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target, decoder, encoder, tokenizer, loss_object, optimizer): # input from img tensor, n target word captions\n",
    "  loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0]) # zeros(64x512)\n",
    "\n",
    "  # word index <start> -> 2 \n",
    "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1) # decide input\n",
    "  \n",
    "  # print(\"target.shape[0]: \", target.shape[0])\n",
    "  # target.shape[0]:  64 (batch size) 64 x 13\n",
    "  # print(\"dec_input: \", dec_input)\n",
    "  # dec_input:  Tensor(\"ExpandDims:0\", shape=(64, 1), dtype=int32)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)  # 64 x 64 x 2048 --> 64 x 64 x 256\n",
    "      # print(\"target.shape[1]: \", target.shape[1])\n",
    "      # target.shape[1]:  13\n",
    "\n",
    "      for i in range(1, target.shape[1]): # loop over the target 1-13 times (1 word 1 loop)\n",
    "          # passing the features through the decoder\n",
    "          # overwrite hidden from attention\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "          # print(i)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions, loss_object)\n",
    "          # print(\"target[:, i]: \", target[:, i])\n",
    "\n",
    "          # using teacher forcing # overwrite new decide_input\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  # backpropagation\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image, max_length, decoder, encoder, tokenizer, image_features_extract_model):\n",
    "    # np zeros 13x64\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    # tf zeros 1x512\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    # load img resize n normalize 299x299x3 (`-1` - `1`) entry\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "  # collect cap img into variable  \n",
    "  train_captions, img_name_vector = collect_capImg(annotations)\n",
    "  # init img feature extractor model\n",
    "  image_features_extract_model = img_featExtract()\n",
    "  # cek summary feature extractor model\n",
    "  # image_features_extract_model.summary()\n",
    "\n",
    "  # done caching you can comment this one below\n",
    "  cache_feature(img_name_vector, image_features_extract_model)\n",
    "  \n",
    "  tokenizer = tokenize_cap(train_captions)\n",
    "  cap_vector = vectorize_cap(tokenizer, train_captions)\n",
    "  max_length = calc_max_length(tokenizer.texts_to_sequences(train_captions))\n",
    "  print(max_length)\n",
    "\n",
    "  # Create training and validation sets using an 80-20 split '384 - 96'\n",
    "  img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
    "                                                                      cap_vector,\n",
    "                                                                      test_size=0.2,\n",
    "                                                                      random_state=0)\n",
    "\n",
    "  print('img_name_train: ', len(img_name_train), 'cap_train: ', len(cap_train), 'img_name_val: ', len(img_name_val), 'cap_val: ', len(cap_val))\n",
    "\n",
    "  dataset = load_npFile(img_name_train, cap_train, BATCH_SIZE, BUFFER_SIZE)\n",
    "\n",
    "  encoder = CNN_Encoder(embedding_dim)\n",
    "  decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')\n",
    "\n",
    "  ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                            decoder=decoder,\n",
    "                            optimizer = optimizer)\n",
    "\n",
    "  ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "  start_epoch = 0\n",
    "\n",
    "  if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "\n",
    "  # adding this in a separate cell because if you run the training cell\n",
    "  # many times, the loss_plot array will be reset\n",
    "  loss_plot = []\n",
    "\n",
    "  for epoch in range(start_epoch, EPOCHS):\n",
    "      start = time.time()\n",
    "      total_loss = 0\n",
    "\n",
    "      for (batch, (img_tensor, target)) in enumerate(dataset): # 384 / 64 = 6 step or 6 batch\n",
    "          batch_loss, t_loss = train_step(img_tensor, target, decoder, encoder, tokenizer, loss_object, optimizer)\n",
    "          total_loss += t_loss\n",
    "\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "      # storing the epoch end loss value to plot later\n",
    "      loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "      if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "      print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                          total_loss/num_steps))\n",
    "      print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "  plt.plot(loss_plot)\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Loss Plot')\n",
    "  plt.show()\n",
    "\n",
    "  # captions on the validation set\n",
    "  rid = np.random.randint(0, len(img_name_val))\n",
    "  image = img_name_val[rid]\n",
    "  real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "  result, attention_plot = evaluate(image, max_length, decoder, encoder, tokenizer, image_features_extract_model)\n",
    "\n",
    "  print ('Real Caption:', real_caption)\n",
    "  print ('Prediction Caption:', ' '.join(result))\n",
    "  plot_attention(image, result, attention_plot)\n",
    "\n",
    "  image_path = PATH + '/data_test/IC5.jpg'\n",
    "  # image_extension = image_url[-4:]\n",
    "  # image_path = tf.keras.utils.get_file('image'+image_extension,\n",
    "  #                                      origin=image_url)\n",
    "\n",
    "  result, attention_plot = evaluate(image_path, max_length, decoder, encoder, tokenizer, image_features_extract_model)\n",
    "  print ('Prediction Caption:', ' '.join(result))\n",
    "  plot_attention(image_path, result, attention_plot)\n",
    "  # opening the image\n",
    "  Image.open(image_path)\n",
    "\n",
    "  image_path = PATH + '/data_test/IC4.jpg'\n",
    "  # image_extension = image_url[-4:]\n",
    "  # image_path = tf.keras.utils.get_file('image'+image_extension,\n",
    "  #                                      origin=image_url)\n",
    "\n",
    "  result, attention_plot = evaluate(image_path, max_length, decoder, encoder, tokenizer, image_features_extract_model)\n",
    "  print ('Prediction Caption:', ' '.join(result))\n",
    "  plot_attention(image_path, result, attention_plot)\n",
    "  # opening the image\n",
    "  Image.open(image_path)\n",
    "\n",
    "  image_path = PATH + '/data_test/IC3.JPG'\n",
    "  # image_extension = image_url[-4:]\n",
    "  # image_path = tf.keras.utils.get_file('image'+image_extension,\n",
    "  #                                      origin=image_url)\n",
    "\n",
    "  result, attention_plot = evaluate(image_path, max_length, decoder, encoder, tokenizer, image_features_extract_model)\n",
    "  print ('Prediction Caption:', ' '.join(result))\n",
    "  plot_attention(image_path, result, attention_plot)\n",
    "  # opening the image\n",
    "  Image.open(image_path)\n",
    "\n",
    "  image_path = PATH + '/data_test/IC2.jpg'\n",
    "  # image_extension = image_url[-4:]\n",
    "  # image_path = tf.keras.utils.get_file('image'+image_extension,\n",
    "  #                                      origin=image_url)\n",
    "\n",
    "  result, attention_plot = evaluate(image_path, max_length, decoder, encoder, tokenizer, image_features_extract_model)\n",
    "  print ('Prediction Caption:', ' '.join(result))\n",
    "  plot_attention(image_path, result, attention_plot)\n",
    "  # opening the image\n",
    "  Image.open(image_path)\n",
    "\n",
    "  image_path = PATH + '/data_test/IC1.jpg'\n",
    "  # image_extension = image_url[-4:]\n",
    "  # image_path = tf.keras.utils.get_file('image'+image_extension,\n",
    "  #                                      origin=image_url)\n",
    "\n",
    "  result, attention_plot = evaluate(image_path, max_length, decoder, encoder, tokenizer, image_features_extract_model)\n",
    "  print ('Prediction Caption:', ' '.join(result))\n",
    "  plot_attention(image_path, result, attention_plot)\n",
    "  # opening the image\n",
    "  Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
